# COMS6998E-ML Cloud
early ai stirs excitement, ml begins to flourish, dl breakthru drive ai bloom, <br><br>
	- ml history: 1943 first math model of nn presented, 1949 theories on how behavior relates to nn and brain activity presented, 1950 the turing test to determine if a computer has real intelligence, 1952 the first ever computing learning program the game of checkers and ibm computer improved at the game the more it played, 1957 perceptron the first nn for computers, 1967 nearest neighbor algorithm, 1980 multilayer perceptron do not have significant different from dnn today, 1986 backprop usually more than 3 hidden layers is not working well, 1989 dnn with one single hidden layer can model any func, 2006 restricted boltzmann machine initialization a generative stochastic artificial nn that can learn a prob distribution over its set of inputs, 2009 gpu; ml is to give computers the ability to learn without being explicitly programmed, a computer program is said to to learn from experience wrt some task and some perf measure, if its perf on task as measured improves with experience, such as character recognition task is recognize a character experience is data with pic of the character and the value perf is accuracy; traditional programming vs ml that former is data/input program -> -> output latter is data output -> -> program/model (training phase) program input -> -> output (inference phase)<br>
	- ml classification by learning paradigms :supervised vs unsupervised: former that input data called training data with a known label or result, a model is constructed thru training by using training data set improved by receiving feedback predictions, the learning process continues until the model achieves a desired level of accuracy on the training data future incoming data without known labels is tested on the model with an acceptable level of accuracy, latter is all input data are not labeled with a known result, a model is generated by exploring the hidden structures present in input data to extract general rules go thru a math process to reduce redundancy or organize data by similarity testing; supervised learning including :linear regression, logistic regression, svm, supervised learning can be widely used in the cc op: linear regression that identifies relationship bt a continuous dependent var and one or more independent var, predicts a continuous var, plots a line of best fit thru a set of data pts which is typically calculated using the least squares method, logistic regression that estimates prob of an event occurring based on a given dataset of independent var, predicts a categorical var, needs an adequate sample to represent values across all the response categories, is used in fraud detection disease prediction etc, svm find a hyperplane in an n-dim space n is  #features that classifies data inputs, is used in various classifications, (logistic regression vs svm that logistic regression have different decision boundaries with different weights that are near the optimal pt, while svm tries to find the best margin i.e. distance bt the line and the support vectors that separates the classes and this reduces risk of err on the data, logistic regression works with already identified independent var while svm works well with unstructured semi-structured data like txt/img, logistic regression is based on statistical approach while svm is based on geometrical properties of data, logistic regression is vulnerable to overfitting while svm is less), learning can be widely used in the cc op including resource allocation anomaly detection predictive maintenance cost prediction load balancing energy efficiency perf monitoring; unsupervised learning including :clustering, principal component analysis: clustering is given a lot of input data with no labels and tries to find groupings in the data, principal component analysis is reduce dim by transforming a large set of var into a smaller one that still contains most of the info in the large set; ml including :decision trees, rl: decision trees is a non-parametric supervised learning algorithm which is utilized for both classification and regression tasks it has a hierarchical tree structure which consists of a root node branches internal nodes leaf nodes, rl like q-learning teaches an agent to make decisions by learning an action-value func thru trial-and-err experiences to learn, advantage lies in model-free no prior knowledge and flexible, disadvantage lies in need balance bt exploration vs exploitation maybe need large data hence learning can be slow<br>
	- ml steps :nn: ml equals search for a func, different types of func including regression and classification former is the func outputs a number latter is the func outputs a correction option, procedure 1.func with unknown params, 2.define loss of training data, 3.optimization of gd & updates<br>
	- speech recognition better than humans?[1][2][3][4] image recognition better than humans?[1] reading comprehension better than humans?[1][2] gaming/strategy better than humans?[1][2] image recognition better than humans?[1][2]<br>
	- chatgpt generative pretrained trasformer key tech is self supervised learning, it is not canned response prepared by the developer different answer for same questions, it is not using internet search results originally most answers cannot find the identical one from internet web-enabled mode can perform realtime searches to get up2date info, it is like a word chain game, it is given input text predicts the next word, chatgpt related research including :core model tech, security privacy misuse, ethics policy governance: core model tech including prompt engineering i.e. study the effects of different prompts on model behavior and perf, model scaling and efficiency i.e. how model size data compute affect capabilities and costs, safety and robustness i.e. making responses more reliable and reducing hallucinations/harmful outputs, security privacy misuse including security vulnerabilities i.e. how llm can be exploited like data extraction attacks, malicious use cases i.e. gen of disinfo scams social media manipulation, privacy risks i.e. what user data might be inadvertently exposed or inferred, ethics policy governance i.e. fair use bias mitigation regulation frameworks, detect objects generated by ai<br>
	- what is missing in the real applications? scalability, efficiency, hw: model efficiency that techniques like pruning quantization knowledge distillation for deployment on edge devices like tinyml, energy efficient ml that reducing computational ftprints like green ai and leveraging neuromorphic computing for brain inspired hw, quantum ml that exploring quantum algorithms for optimization sampling solving classically intractable problems; trustworthy ethical ai: explainability(xAI) that developing  methods to interpret complex models like attention virtualization concept based explanations, bias/fairness mitigation that debiasing datasets and algorithms to ensure equitable outcomes, ai safety and alignment that ensuring systems align with human values like rlhf, robustness adversarial defense that protecting models against attacks like adversarial[1][2][3] training certified robustness<br>
	- can machine know I do not know :machines can admit uncertainty, machines do not know in the human sense, advanced ai and probabilistic I do not know: machines can admit uncertainty that prob based models i.e. if a model assigns low confidence to all options it can respond I do not know instead of guessing, rule based sys i.e. if none of the rules match a query they can output I do not know, knowledge checking systems i.e. chatbots often have fallback responses when a query is outside their knowledge base, machines do not know in the human sense that do not have consciousness or understanding, only compute likelihoods match patterns check rules, advanced ai and probabilistic I do not know that llm can access confidence in generated answers, some sys use uncertainty thresholds to refuse to answer, in scientific applications ai might highlight gaps in knowledge or inconclusive data which looks like I do not know; explain why I know is explainable ai; machine illusion adversarial attack, how to prevent adversarial attack?; ml vs life long learning/continual that former is train a model once on a fixed dataset, after training the model is deployed, new data requires retraining from scratch or finetuning which can be inefficient, latter is the model keeps learning incrementally as new data arrives, adapts to new tasks without losing perf on previous tasks, mimics how humans learn continuously thruout life, key challenges including difficult due to catastrophic forgetting, when a model learns a new task it often forgets previously learned tasks, maintaining old knowledge while learning new knowledge is a central problem, techniques including regularization based methods i.e. penalize changes to param that are important for old tasks, replay based methods i.e. store examples of old tasks(mem) and replay them while learning new tasks, dynamic architectures i.e. expand the network for new tasks while keeping old parts intact, meta-learning approaches i.e. train the model to learn how to learn so it can quickly adapt to new tasks with minimal forgetting, applications including robotics i.e. robots improve skills over yrs without losing old skills, recommendation sys i.e. continuously adapt to changing user behavior, nn processing i.e. models adapt to new words slang knowledge, av i.e. continuously learn from new environments and situations; does it require a lot of training data? few shot learning, zero-shot learning; rl<br>
